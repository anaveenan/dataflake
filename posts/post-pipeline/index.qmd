---
title: "Decorators"
author: "Naveenan Arjunan"
date: "2024-03-02"
categories: [python]
draft: true

---


```{python, eval=FALSE}
import numpy as np
from sklearn.datasets import fetch_openml

X, y = fetch_openml("titanic", version=1, as_frame=True, return_X_y=True)
```

```{python, eval=FALSE}
from sklearn.base import BaseEstimator,TransformerMixin
from sklearn.pipeline import Pipeline,make_pipeline,make_union
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder,Binarizer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score,RandomizedSearchCV

class SelectCols(BaseEstimator, TransformerMixin):

    def __init__(self,columns) -> None:
        self.columns = columns

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[self.columns]

# feat_pipe=make_pipeline(SelectCols(['pclass','sex']),OneHotEncoder(sparse=False))

# feat_pipe=make_union(
#     make_pipeline(SelectCols(['pclass','sex']),OneHotEncoder(sparse=False)),
#     make_pipeline(SelectCols(['age'])
#     ,SimpleImputer(fill_value=19, strategy="constant")
#     ,make_union(Binarizer(threshold=18),Binarizer(threshold=12))),
#     SelectCols(['age','fare'])
# )

# pipe=make_pipeline(feat_pipe,HistGradientBoostingClassifier())

# feat_pipe.fit_transform(X)

# cross_val_score(pipe,X,y,cv=5,n_jobs=2)


# grid_rand = RandomizedSearchCV(
#     HistGradientBoostingClassifier(),
#     n_iter=100,
#     param_distributions={
#         "max_iter": np.linspace(10, 200, 100).astype(int),
#         "l2_regularization": np.linspace(0, 4, 50)
#     },
#     scoring='accuracy',  # Use accuracy as the scoring metric
#     cv=5,
#     n_jobs=-1
# )

from scipy.optimize import minimize
import numpy as np 

# # Generate synthetic data for linear regression with higher dimensions
# np.random.seed(42)
# n_samples = 100
# n_features = 3  # Example with 3 features
# X = np.random.rand(n_samples, n_features) * 10  # Features
# noise = np.random.randn(n_samples) * 2  # Noise
# true_coeffs = np.array([2, 1, -0.5])  # True coefficients (example)
# y = X @ true_coeffs + noise  # Target (with noise)
 
# def linear_regression_loss(params,X,y):
#     return np.sum((y-X@params)**2)

# def quantile_loss(params,X,y,quantile):
#     residual=y-X@params
#     return np.sum(np.where(residual>0,quantile*residual,(1-quantile)*residual))

# result= minimize(fun=linear_regression_loss,
# x0=np.zeros(X.shape[1]),
# args=(X,y),
# method='BFGS'
# )


# def linear_regression_loss(params,X,y):
#     return np.sum((y-X@params)**2)

# def quantile_loss(params,X,y,quantile):
#     residual=y-X@params
#     return np.sum(np.where(residual>0,quantiles*residual,(1-quantile)*residual))


# minimize(linear_regression_loss,
# x0=np.zeros(X.shape[1]),
# args=(X,y),
# method='BFGS'
# )

# result= minimize(fun=quantile_loss,
# x0=np.zeros(X.shape[1]),
# args=(X,y,0.5),
# method='BFGS'
# )

# result

# result



```

```{python, eval=FALSE}
from sklearn.pipeline import Pipeline,make_pipeline,make_union
from sklearn.base import BaseEstimator,TransformerMixin
from sklearn.preprocessing import OneHotEncoder
import pandas as pd 

class SelectCols(BaseEstimator,TransformerMixin):

    def __init__(self,columns) -> None:
        self.columns = columns

    def fit(self,X,y=None):
        return self
    
    def transform(self,X):
        return X[self.columns]


# pipe=Pipeline(
#     [
#         ('pclass_col',SelectCols(["pclass"])),
#         ('pclass_enc',OneHotEncoder(sparse=False)),
#     ]
# )

# pipe.fit_transform(X)
# pipe.get_params()


# feat_pipe=make_union(
#     make_pipeline (SelectCols(["pclass"]),OneHotEncoder(sparse=False))
#     ,SelectCols(["age"])
#     )


# pipe=make_pipeline(feat_pipe,HistGradientBoostingClassifier())

# pipe.fit_transform(X)

# pipe.get_params()
```



```{python}
from lmfit import Minimizer, Parameters, report_fit
import numpy as np
import matplotlib.pylab as plt
```



```{python, eval=FALSE}
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

# # Define the number of samples (cab rides) in the dataset
# num_samples = 1000

# # Generate random geospatial data (latitude and longitude)
# np.random.seed(42)  # For reproducibility
# pickup_latitude = np.random.uniform(40.5, 41, num_samples)
# pickup_longitude = np.random.uniform(-74.1, -73.8, num_samples)
# dropoff_latitude = np.random.uniform(40.5, 41, num_samples)
# dropoff_longitude = np.random.uniform(-74.1, -73.8, num_samples)

# # Generate random temporal data (pickup datetime)
# start_date = datetime(2016, 1, 1)
# end_date = datetime(2016, 6, 30)
# pickup_datetime = [start_date + timedelta(minutes=np.random.randint(0, int((end_date - start_date).total_seconds())//60)) for _ in range(num_samples)]

# # Generate random categorical data (vendor_id and store_and_fwd_flag)
# vendor_ids = np.random.choice(['VENDOR_1', 'VENDOR_2'], num_samples)
# store_and_fwd_flag = np.random.choice(['Y', 'N'], num_samples)

# # Generate random discrete data (passenger_count)
# passenger_count = np.random.randint(1, 7, num_samples)

# # Generate random trip duration as the target/label column
# trip_duration = np.random.randint(300, 3600, num_samples)  # Random duration between 5 minutes to 1 hour

# # Create a DataFrame to store the generated sample data
# data = {
#     'pickup_datetime': pickup_datetime,
#     'pickup_latitude': pickup_latitude,
#     'pickup_longitude': pickup_longitude,
#     'dropoff_latitude': dropoff_latitude,
#     'dropoff_longitude': dropoff_longitude,
#     'vendor_id': vendor_ids,
#     'store_and_fwd_flag': store_and_fwd_flag,
#     'passenger_count': passenger_count,
#     'trip_duration': trip_duration
# }
# df = pd.DataFrame(data)

# # Print the first few rows of the generated sample data
# print(df.head())

```


```{python, eval=FALSE}
# df.head() 
```



```{python, eval=FALSE}
# import numpy as np
# import matplotlib.pyplot as plt
# plt.rcParams["figure.figsize"] = (5, 3) # (w, h)
# plt.rcParams["figure.dpi"] = 200


# m = 100
# x = 2 * np.random.rand(m)
# y = 5 + 2 * x + np.random.randn(m)
# plt.scatter(x, y);
# from scipy.stats import linregress

# slope,intercept=linregress(x,y)[:2]
# print(f"slope:{slope:.2f} intercept:{intercept:.2f}")

# def mseloss(theta,X,y):
#     return -1 * (1/len(y))* np.sum((X@theta-y)**2) 
# x.shape

# np.column_stack([np.ones(len(x)),x])

# from scipy.stats import linregress 

# slope,intercept=linregress(x,y)[:2]
# print(f"slope:{slope:.2f} intercept:{intercept:.2f}")

# x = np.array([np.ones(m), x]).transpose()
# def accuracy(x, y, theta):
#     return - 1 / m * np.sum((np.dot(x, theta) - y) ** 2)

# def gradient(x, y, theta):
#     return -1 / m * x.T.dot(np.dot(x, theta) - y)

# num_epochs = 500
# learning_rate = 0.1

# def train(x, y):
#     accs = []
#     thetas = []
#     theta = np.zeros(2)
#     for _ in range(num_epochs):
#         # keep track of accuracy and theta over time
#         acc = accuracy(x, y, theta)
#         thetas.append(theta)
#         accs.append(acc)
        
#         # update theta
#         theta = theta + learning_rate * gradient(x, y, theta)
        
#     return theta, thetas, accs

# theta, thetas, accs = train(x, y)
# print(f"slope: {theta[1]:.3f}, intercept: {theta[0]:.3f}")
# slope: 1.899, intercept: 5.128  

# plt.plot(accs)
# plt.xlabel('Epoch Number')
# plt.ylabel('Accuracy');


# from mpl_toolkits.mplot3d import Axes3D
# i = np.linspace(-10, 20, 50)
# j = np.linspace(-10, 20, 50)
# i, j = np.meshgrid(i, j)
# k = np.array([accuracy(x, y, th) for th in zip(np.ravel(i), np.ravel(j))]).reshape(i.shape)
# fig = plt.figure(figsize=(9,6))
# ax = fig.gca(projection='3d')
# ax.plot_surface(i, j, k, alpha=0.2)
# ax.plot([t[0] for t in thetas], [t[1] for t in thetas], accs, marker="o", markersize=3, alpha=0.1);
# ax.set_xlabel(r'$\theta_0$'); ax.set_ylabel(r'$\theta_1$')
# ax.set_zlabel("Accuracy");
```


```{python, eval=FALSE}
# import duckdb

# with duckdb.connect(":memory:") as con:
#     con.execute("SET s3_endpoint='storage.googleapis.com'")
#     updates = con.execute(f"""
#     SELECT *
#     FROM READ_PARQUET('s3://bike-sharing-history/toulouse/jcdecaux/*/*.parquet');
#     """).fetch_df()  # this is a pandas DataFrame

# updates.head()

# history = (
#     updates
#     .groupby('station')
#     .resample(
#         rule='15min',
#         on='commit_at'
#     ).max()
#     .drop(columns='station')
#     .groupby('station').shift(1)
#     .groupby('station').ffill()
#     .dropna(subset=['skipped_updates'])
#     [['bikes', 'stands']].astype('uint8')
# )




# bikes_ahead = (
#     history
#     .groupby('station')['bikes'].shift([-i for i in range(1, 9)])
#     .dropna()
#     .astype('uint8')
# )
# bikes_ahead.columns = [f'bikes_+{15 * i}min' for i in range(1, 9)]

# print(
#     bikes_ahead
#     .query("station == '00003 - POMME'")
#     .head()
#     .to_markdown()
# )

# history.loc[bikes_ahead.index]

# from sklearn import ensemble
# from sklearn import model_selection

# model = ensemble.HistGradientBoostingRegressor(
#     loss='poisson',
#     random_state=42
# )
# features = history.loc[bikes_ahead.index]

# cv = model_selection.GroupKFold(n_splits=5)
# groups = bikes_ahead.index.get_level_values('station')

# features.head()
# bikes_ahead.columns


# features = history.loc[bikes_ahead.index]

# cv = model_selection.GroupKFold(n_splits=5)
# groups = bikes_ahead.index.get_level_values('station')

# for target_col in bikes_ahead.columns:
#     scores = model_selection.cross_val_score(
#         model,
#         features,
#         bikes_ahead[target_col],
#         groups=groups,
#         cv=cv,
#         scoring='neg_mean_squared_error'
#     )
#     print(f'{target_col} — {-scores.mean():.2f} ± {scores.std():.2f}')


# import numpy as np
# import scipy as sp 
# from joblib import dump
# from pathlib import Path
# import polars as pl

# def calcbytes(s):
#     arr=np.zeros((s,1))
#     arr[s-1]=1
#     dump(arr,'tmp.pickle')
#     return Path('tmp.pickle').stat().st_size

# s = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]
# b= [ calcbytes(s) for _ in s]
# b

# s
```

1. Setting Environment Variables:  
`nano .zshenv`
` export EIA_API_KEY = '' `

2. 

Data pipeline 
Process of moving data from one data source to another
ETL 

Scope: 
1. Hourly demand for electricy by subregrion 
2. All subregions unded california system opertor: 
    a) Pacific Gas and Electric 
    b) San Diego Gas ane Electric 
    c) Southern California Edison 
    d) Valley Electric Association 
3. Refresh daily 

Requirement: 
1. Fully automated 
2. High level of customization
3. Data quality and unit tests 
4. Monitor 

Data pipeline Architecture: 

<!-- ![Caption for the picture.](/path/to/image.png) -->

1. Data processing 
2. Create and udpate metadata 
3. Append data to the normalized table 
https://api.eia.gov/v2/electricity/rto/region-sub-ba-data/data/?frequency=hourly&data[0]=value&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=fwfOQIBXl0JIpVhL0QveMOuWmtIs169EP72zugF1



Data backfill - runned locally


Data Quality - unit tests 

Deterministic 
1. Data structure and attributes
2. Field names 
3. Value ranges 
4. Duplicates

Non-Deterministic

1. Missing values
2. Value ranges
3. Delays 

Github Actions

Trigger vs Scheduled Action

Github secrets 
Logs 
Docker 

We need a YAML 
OS 
Docker - reproducable environment
workflow 

.github/workflow



```{python}
import requests 

```


I was tasked with creating a view that displayed the total scan count for each employee along with the date of their latest scan. Here's a glimpse of what I encountered:

Given a demo table 'employees' with columns:

employee_id (int)
employee_card_scan_count (int)
employee_card_scan_date (string)

The desired view 'employee_scan_count' was to contain:

employee_id
total_scans
latest_scan_date

In my exploration, I first attempted to find the total scan count for each employee:

Query 1:

SELECT employee_id, COUNT(employee_card_scan_count) AS total_scans
FROM employees
GROUP BY employee_id
This query executed in around a minute.

However, when I added the additional requirement to find the latest scan date:

Query 2:

SELECT employee_id, SUM(employee_card_scan_count) AS total_scans, MAX(employee_card_scan_date) AS latest_scan_date
FROM employees
GROUP BY employee_id

I was astonished to find that this query took nearly 5 minutes to run – five times the time taken by Query 1.

After delving deeper into the internals of Spark SQL, I uncovered the reason behind this stark difference in execution times:

In Query 1, Spark utilized a hash table to store 'employee_id' as the key and updated it whenever the same key was encountered during table scanning. This resulted in a time complexity of O(n).

However, in Query 2, when Spark attempted the same mechanism for the key-value pair of {employee_id, total_scans, latest_scan_date}, it encountered a bottleneck. Since 'latest_scan_date' was of type string and strings are immutable, Spark couldn't perform updates on the value efficiently. Consequently, it employed a technique called sort aggregation. This involved sorting the entire table based on 'employee_id' before performing the group by operation, resulting in a time complexity of O(nlog(n)).

In the realm of big data, the impact of this logarithmic complexity can be significant.

For instance, with a total of n = 100,000 rows:

O(n) = 100,000
O(nlog(n)) = 100,000 * log(100,000) ≈ 500,000
Hence, the additional execution time for Query 2 was attributed to this discrepancy in complexity.

For your knowledge, query 1 used Hash aggregation mechanism and query 2 used Sort aggregation.

With this puzzle solved, I was able to rest easy, content with a deeper understanding of Spark SQL's optimization strategies – and a well-deserved three-hour nap, as it was already 6 AM.



