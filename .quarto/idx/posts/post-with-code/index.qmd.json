{"title":"The Pandas Reference","markdown":{"yaml":{"title":"The Pandas Reference","author":"Naveenan Arjunan","date":"2024-03-02","categories":["pandas","sql","analysis"],"image":"image.jpg"},"headingText":"About","containsRefs":false,"markdown":"\n\n\n\nIn the vast world of data, it's common to encounter information organized in a rectangular format, characterized by its rows and columns. This type of data goes by various names—whether it's referred to as table data, data frames, structured data, spreadsheets, or through the lens of the Python library, Pandas, known for data manipulation for structured datasets. This blog post dives into the essential operations integral to any data analysis project, mirroring SQL equivalents such as selecting column references, scalar expressions, applying conditions with 'where', grouping, aggregating, ordering, utilizing window functions, and joining datasets.\n\nEmbarking on my journey with Pandas, I quickly noticed the plethora of methods available for executing identical tasks. Moreover, the transition from the succinctness of SQL queries to Pandas code often resulted in less elegant, challenging-to-debug scripts. Through this post, I aim to demystify the process of performing these cornerstone SQL operations within Pandas, guiding you towards crafting code that's not only readable but also straightforward to maintain. Join me as we explore practical examples to elevate your data manipulation skills in Python.\n\n\n```{python}\n#| echo: true\n#| warning: false\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\")\npd.options.display.max_rows = 20\ndf.head(5)\n```\n\n## Select columns\n\nTo select specific columns, employ the .loc method combined with a list of the column names you wish to include. I suggest adopting this approach because it grants enhanced flexibility for your data analysis endeavors.\n\n`.loc[:,['col1','col2']]`\n\nFor example, to extract the total_bill and tips columns from your dataset, utilize method chaining to execute these operations sequentially. This technique allows for a streamlined and efficient workflow.\n\n\n```{python}\n(df\n .loc[:,['tip','sex']]\n .head()\n)\n```\n\nSelect columns that begin with the letter 't' by employing a straightforward and intelligible syntax. This method simplifies executing complex selection tasks in Pandas, making your data analysis more efficient.\n\n```{python}\n(df\n .loc[:,[col for col in df.columns if col.startswith('t')]]\n .head()\n)\n```\n\n## Select columns manipulation\n\nTo create new columns or modify existing ones, the .assign method is your go-to. This approach not only allows for the addition of new columns but also the updating of existing ones in a concise manner.\n\nTo add a new column with a constant value\n`.assign(new_col=1)`\nTo introduce a new column based on operations with existing columns: \n`.assign(new_col=lambda x:x['col']+1)`\nTo update an existing column by modifying its values: \n`.assign(old_col=lambda x:x['old_col']+1)`\n\n\n```{python}\n(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill']) #add new column\n .assign(tip=lambda x:x['tip']+1) # update existing column \n .assign(count=1) #add constant value \n .head()\n)\n```\n\n\n## Filter rows (where)\n\nUtilize the query method to filter row in a pandas Dataframe \n\n`\nval=10\n.query(\"col1>='10'\")\n.query(\"col1>='@val'\")\n.query(f\"col1>='{val}'\")\n.query(\"col1.isin(['a','b'])\",engine='python')\n`\n\n```{python}\n#filter only transaction with more than 15% in tips\n(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill'])\n .query(\"percentage_tip>.15\")\n .head()\n)\n```\n\n\n```{python}\nper_tip=.15\n#using @ within query to refer a variable in the filter \nprint(\"\")\ndisplay(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill'])\n .query(\"percentage_tip>@per_tip\")\n .head()\n)\n\n#using f-string to perform filtering\ndisplay(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill'])\n .query(f\"percentage_tip>{per_tip}\")\n .head()\n)\n```\n\n\n```{python}\n#Filter only transactions happend on Sunday and Monday\n(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .query(\"day.isin(['Sun','Mon'])\",engine='python')\n .head()\n)\n```\n\n## Group By and Aggregation\n\nLeverage `groupby` along with `named aggs` for versatile aggregations. The aggregation functions accommodate lambda expressions and numpy operations for comprehensive data analysis.\n\n\n```{python}\n#By day get average and total bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum')) #multiple column aggregations supported\n .reset_index()\n)\n```\n\n\n```{python}\n#By day get average of total bill using : functions, lambda functions, numpy functions \n(df\n .groupby(['day'])\n .agg(avg_bill_mean=('total_bill','mean')\n     ,avg_bill_lambda=('total_bill',lambda x:x.mean()) #using lambda functions\n     ,avg_bill_np=('total_bill',np.mean)) #using numpy functions \n .reset_index()\n)\n```\n\n## Ordering rows\n\nSorting is often a necessary step in data analysis, either as a preparatory action or to organize the final output. In pandas, this is achieved through the sort_values function, which orders the DataFrame by specified columns or axes.\n\n.sort_values(['col1','col2'],ascending=[True,False])\n\n\n```{python}\n#By day get average and total bill.Sort the output by total_bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum'))\n .reset_index()\n .sort_values(['total_bill']) #Default in ascending \n)\n```\n\n\n```{python}\n#By day get average and total bill.Sort the output by total_bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum'))\n .reset_index()\n .sort_values(['total_bill'],ascending=[False]) #By descending order \n)\n```\n\n\n```{python}\n#By day get average and total bill.Sort the output by total_bill and avg_bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum'))\n .reset_index()\n .sort_values(['total_bill','avg_bill'],ascending=[False,True]) #By multiple columns one by asc and other by desc\n)\n```\n\n## Window function\n\nWindow functions in SQL offering advanced data manipulation capabilities. We will explore how to utilize key functions such as `row_number()`, `LEAD()`/`LAG()`, and calculate a running sum within each group (partition).\n\n\n```{python}\n#Equivalent of row_number() over(partition by day order by total_bill asc) as row_number\n(df\n .assign(row_number=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day']).cumcount()+1)\n .sort_values(['row_number'])\n .head()\n)\n```\n\n\n```{python}\n#Equivalent of lag(total_bill) over(partition by day order by total_bill asc) as previous_bill\n(df\n .assign(row_number=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day']).cumcount()+1)\n .assign(prev_bill=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day'])['total_bill'].shift(1))\n .sort_values(['row_number'])\n .head()\n)\n```\n\n\n```{python}\n#Equivalent of lead(total_bill) over(partition by day order by total_bill asc) as previous_bill\n(df\n .assign(row_number=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day']).cumcount()+1)\n .assign(next_bill=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day'])['total_bill'].shift(-1))\n .sort_values(['row_number'])\n .head()\n)\n```\n\n\n```{python}\n#Equivalent of sum(total_bill) over(partition by day) as sum_bill_day\n#Equivalent of sum(tip) over(partition by day order by total_bill asc) as cum_tip_day\n#Equivalent of sum(tip) over(partition by day order by total_bill rows between 3 preceeding and current row) as rolling_3d_sum \n\n(df\n .assign(sum_bill_day=lambda x:x.groupby(['day'])['total_bill'].transform('sum'))\n .assign(cum_tip_day=lambda x:x.sort_values(['total_bill']).groupby(['day'])['tip'].cumsum())\n .assign(rolling_3d_sum=lambda x:x.sort_values(['total_bill']).groupby(['day'])['tip'].rolling(window=2,min_periods=1).sum().reset_index(drop=True, level=0))\n .query(\"day=='Sat'\")\n .sort_values(['total_bill'])\n .head()\n)\n```\n\n## Conclusion\n\nThrough this blog post, I've provided a few straightforward strategies to enhance the efficiency of data analysis projects. I intend to enrich this post with additional examples, aiming to simplify data analysis with pandas further.","srcMarkdownNoYaml":"\n\n\n## About\n\nIn the vast world of data, it's common to encounter information organized in a rectangular format, characterized by its rows and columns. This type of data goes by various names—whether it's referred to as table data, data frames, structured data, spreadsheets, or through the lens of the Python library, Pandas, known for data manipulation for structured datasets. This blog post dives into the essential operations integral to any data analysis project, mirroring SQL equivalents such as selecting column references, scalar expressions, applying conditions with 'where', grouping, aggregating, ordering, utilizing window functions, and joining datasets.\n\nEmbarking on my journey with Pandas, I quickly noticed the plethora of methods available for executing identical tasks. Moreover, the transition from the succinctness of SQL queries to Pandas code often resulted in less elegant, challenging-to-debug scripts. Through this post, I aim to demystify the process of performing these cornerstone SQL operations within Pandas, guiding you towards crafting code that's not only readable but also straightforward to maintain. Join me as we explore practical examples to elevate your data manipulation skills in Python.\n\n\n```{python}\n#| echo: true\n#| warning: false\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\")\npd.options.display.max_rows = 20\ndf.head(5)\n```\n\n## Select columns\n\nTo select specific columns, employ the .loc method combined with a list of the column names you wish to include. I suggest adopting this approach because it grants enhanced flexibility for your data analysis endeavors.\n\n`.loc[:,['col1','col2']]`\n\nFor example, to extract the total_bill and tips columns from your dataset, utilize method chaining to execute these operations sequentially. This technique allows for a streamlined and efficient workflow.\n\n\n```{python}\n(df\n .loc[:,['tip','sex']]\n .head()\n)\n```\n\nSelect columns that begin with the letter 't' by employing a straightforward and intelligible syntax. This method simplifies executing complex selection tasks in Pandas, making your data analysis more efficient.\n\n```{python}\n(df\n .loc[:,[col for col in df.columns if col.startswith('t')]]\n .head()\n)\n```\n\n## Select columns manipulation\n\nTo create new columns or modify existing ones, the .assign method is your go-to. This approach not only allows for the addition of new columns but also the updating of existing ones in a concise manner.\n\nTo add a new column with a constant value\n`.assign(new_col=1)`\nTo introduce a new column based on operations with existing columns: \n`.assign(new_col=lambda x:x['col']+1)`\nTo update an existing column by modifying its values: \n`.assign(old_col=lambda x:x['old_col']+1)`\n\n\n```{python}\n(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill']) #add new column\n .assign(tip=lambda x:x['tip']+1) # update existing column \n .assign(count=1) #add constant value \n .head()\n)\n```\n\n\n## Filter rows (where)\n\nUtilize the query method to filter row in a pandas Dataframe \n\n`\nval=10\n.query(\"col1>='10'\")\n.query(\"col1>='@val'\")\n.query(f\"col1>='{val}'\")\n.query(\"col1.isin(['a','b'])\",engine='python')\n`\n\n```{python}\n#filter only transaction with more than 15% in tips\n(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill'])\n .query(\"percentage_tip>.15\")\n .head()\n)\n```\n\n\n```{python}\nper_tip=.15\n#using @ within query to refer a variable in the filter \nprint(\"\")\ndisplay(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill'])\n .query(\"percentage_tip>@per_tip\")\n .head()\n)\n\n#using f-string to perform filtering\ndisplay(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .assign(percentage_tip=lambda x:x['tip']/x['total_bill'])\n .query(f\"percentage_tip>{per_tip}\")\n .head()\n)\n```\n\n\n```{python}\n#Filter only transactions happend on Sunday and Monday\n(df\n .loc[:,['total_bill','tip','sex','day','time']]\n .query(\"day.isin(['Sun','Mon'])\",engine='python')\n .head()\n)\n```\n\n## Group By and Aggregation\n\nLeverage `groupby` along with `named aggs` for versatile aggregations. The aggregation functions accommodate lambda expressions and numpy operations for comprehensive data analysis.\n\n\n```{python}\n#By day get average and total bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum')) #multiple column aggregations supported\n .reset_index()\n)\n```\n\n\n```{python}\n#By day get average of total bill using : functions, lambda functions, numpy functions \n(df\n .groupby(['day'])\n .agg(avg_bill_mean=('total_bill','mean')\n     ,avg_bill_lambda=('total_bill',lambda x:x.mean()) #using lambda functions\n     ,avg_bill_np=('total_bill',np.mean)) #using numpy functions \n .reset_index()\n)\n```\n\n## Ordering rows\n\nSorting is often a necessary step in data analysis, either as a preparatory action or to organize the final output. In pandas, this is achieved through the sort_values function, which orders the DataFrame by specified columns or axes.\n\n.sort_values(['col1','col2'],ascending=[True,False])\n\n\n```{python}\n#By day get average and total bill.Sort the output by total_bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum'))\n .reset_index()\n .sort_values(['total_bill']) #Default in ascending \n)\n```\n\n\n```{python}\n#By day get average and total bill.Sort the output by total_bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum'))\n .reset_index()\n .sort_values(['total_bill'],ascending=[False]) #By descending order \n)\n```\n\n\n```{python}\n#By day get average and total bill.Sort the output by total_bill and avg_bill\n(df\n .groupby(['day'])\n .agg(avg_bill=('total_bill','mean')\n     ,total_bill=('total_bill','sum'))\n .reset_index()\n .sort_values(['total_bill','avg_bill'],ascending=[False,True]) #By multiple columns one by asc and other by desc\n)\n```\n\n## Window function\n\nWindow functions in SQL offering advanced data manipulation capabilities. We will explore how to utilize key functions such as `row_number()`, `LEAD()`/`LAG()`, and calculate a running sum within each group (partition).\n\n\n```{python}\n#Equivalent of row_number() over(partition by day order by total_bill asc) as row_number\n(df\n .assign(row_number=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day']).cumcount()+1)\n .sort_values(['row_number'])\n .head()\n)\n```\n\n\n```{python}\n#Equivalent of lag(total_bill) over(partition by day order by total_bill asc) as previous_bill\n(df\n .assign(row_number=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day']).cumcount()+1)\n .assign(prev_bill=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day'])['total_bill'].shift(1))\n .sort_values(['row_number'])\n .head()\n)\n```\n\n\n```{python}\n#Equivalent of lead(total_bill) over(partition by day order by total_bill asc) as previous_bill\n(df\n .assign(row_number=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day']).cumcount()+1)\n .assign(next_bill=lambda x:x.sort_values(['total_bill'],ascending=[True]).groupby(['day'])['total_bill'].shift(-1))\n .sort_values(['row_number'])\n .head()\n)\n```\n\n\n```{python}\n#Equivalent of sum(total_bill) over(partition by day) as sum_bill_day\n#Equivalent of sum(tip) over(partition by day order by total_bill asc) as cum_tip_day\n#Equivalent of sum(tip) over(partition by day order by total_bill rows between 3 preceeding and current row) as rolling_3d_sum \n\n(df\n .assign(sum_bill_day=lambda x:x.groupby(['day'])['total_bill'].transform('sum'))\n .assign(cum_tip_day=lambda x:x.sort_values(['total_bill']).groupby(['day'])['tip'].cumsum())\n .assign(rolling_3d_sum=lambda x:x.sort_values(['total_bill']).groupby(['day'])['tip'].rolling(window=2,min_periods=1).sum().reset_index(drop=True, level=0))\n .query(\"day=='Sat'\")\n .sort_values(['total_bill'])\n .head()\n)\n```\n\n## Conclusion\n\nThrough this blog post, I've provided a few straightforward strategies to enhance the efficiency of data analysis projects. I intend to enrich this post with additional examples, aiming to simplify data analysis with pandas further."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.550","theme":"pulse","title-block-banner":true,"title":"The Pandas Reference","author":"Naveenan Arjunan","date":"2024-03-02","categories":["pandas","sql","analysis"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}