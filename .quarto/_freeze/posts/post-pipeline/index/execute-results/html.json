{
  "hash": "143a8aa935d8950cd0164e2d4e1db667",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Decorators\"\nauthor: \"Naveenan Arjunan\"\ndate: \"2024-03-02\"\ncategories: [python]\ndraft: true\n\n---\n\n::: {#fa04486b .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/naveenanarjunan/Documents/projects/venv39/lib/python3.9/site-packages/sklearn/datasets/_arff_parser.py:195: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n```\n:::\n:::\n\n\n::: {#d7884b89 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.pipeline import Pipeline,make_pipeline,make_union\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.preprocessing import OneHotEncoder,Binarizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score,RandomizedSearchCV\n\nclass SelectCols(BaseEstimator, TransformerMixin):\n\n    def __init__(self,columns) -> None:\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.columns]\n\n# feat_pipe=make_pipeline(SelectCols(['pclass','sex']),OneHotEncoder(sparse=False))\n\n# feat_pipe=make_union(\n#     make_pipeline(SelectCols(['pclass','sex']),OneHotEncoder(sparse=False)),\n#     make_pipeline(SelectCols(['age'])\n#     ,SimpleImputer(fill_value=19, strategy=\"constant\")\n#     ,make_union(Binarizer(threshold=18),Binarizer(threshold=12))),\n#     SelectCols(['age','fare'])\n# )\n\n# pipe=make_pipeline(feat_pipe,HistGradientBoostingClassifier())\n\n# feat_pipe.fit_transform(X)\n\n# cross_val_score(pipe,X,y,cv=5,n_jobs=2)\n\n\n# grid_rand = RandomizedSearchCV(\n#     HistGradientBoostingClassifier(),\n#     n_iter=100,\n#     param_distributions={\n#         \"max_iter\": np.linspace(10, 200, 100).astype(int),\n#         \"l2_regularization\": np.linspace(0, 4, 50)\n#     },\n#     scoring='accuracy',  # Use accuracy as the scoring metric\n#     cv=5,\n#     n_jobs=-1\n# )\n\nfrom scipy.optimize import minimize\nimport numpy as np \n\n# # Generate synthetic data for linear regression with higher dimensions\n# np.random.seed(42)\n# n_samples = 100\n# n_features = 3  # Example with 3 features\n# X = np.random.rand(n_samples, n_features) * 10  # Features\n# noise = np.random.randn(n_samples) * 2  # Noise\n# true_coeffs = np.array([2, 1, -0.5])  # True coefficients (example)\n# y = X @ true_coeffs + noise  # Target (with noise)\n \n# def linear_regression_loss(params,X,y):\n#     return np.sum((y-X@params)**2)\n\n# def quantile_loss(params,X,y,quantile):\n#     residual=y-X@params\n#     return np.sum(np.where(residual>0,quantile*residual,(1-quantile)*residual))\n\n# result= minimize(fun=linear_regression_loss,\n# x0=np.zeros(X.shape[1]),\n# args=(X,y),\n# method='BFGS'\n# )\n\n\n# def linear_regression_loss(params,X,y):\n#     return np.sum((y-X@params)**2)\n\n# def quantile_loss(params,X,y,quantile):\n#     residual=y-X@params\n#     return np.sum(np.where(residual>0,quantiles*residual,(1-quantile)*residual))\n\n\n# minimize(linear_regression_loss,\n# x0=np.zeros(X.shape[1]),\n# args=(X,y),\n# method='BFGS'\n# )\n\n# result= minimize(fun=quantile_loss,\n# x0=np.zeros(X.shape[1]),\n# args=(X,y,0.5),\n# method='BFGS'\n# )\n\n# result\n\n# result\n\n```\n:::\n\n\n::: {#cf8dd711 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.pipeline import Pipeline,make_pipeline,make_union\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd \n\nclass SelectCols(BaseEstimator,TransformerMixin):\n\n    def __init__(self,columns) -> None:\n        self.columns = columns\n\n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        return X[self.columns]\n\n\n# pipe=Pipeline(\n#     [\n#         ('pclass_col',SelectCols([\"pclass\"])),\n#         ('pclass_enc',OneHotEncoder(sparse=False)),\n#     ]\n# )\n\n# pipe.fit_transform(X)\n# pipe.get_params()\n\n\n# feat_pipe=make_union(\n#     make_pipeline (SelectCols([\"pclass\"]),OneHotEncoder(sparse=False))\n#     ,SelectCols([\"age\"])\n#     )\n\n\n# pipe=make_pipeline(feat_pipe,HistGradientBoostingClassifier())\n\n# pipe.fit_transform(X)\n\n# pipe.get_params()\n```\n:::\n\n\n::: {#3402266f .cell execution_count=4}\n``` {.python .cell-code}\nfrom lmfit import Minimizer, Parameters, report_fit\nimport numpy as np\nimport matplotlib.pylab as plt\n```\n:::\n\n\n::: {#56d13664 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# # Define the number of samples (cab rides) in the dataset\n# num_samples = 1000\n\n# # Generate random geospatial data (latitude and longitude)\n# np.random.seed(42)  # For reproducibility\n# pickup_latitude = np.random.uniform(40.5, 41, num_samples)\n# pickup_longitude = np.random.uniform(-74.1, -73.8, num_samples)\n# dropoff_latitude = np.random.uniform(40.5, 41, num_samples)\n# dropoff_longitude = np.random.uniform(-74.1, -73.8, num_samples)\n\n# # Generate random temporal data (pickup datetime)\n# start_date = datetime(2016, 1, 1)\n# end_date = datetime(2016, 6, 30)\n# pickup_datetime = [start_date + timedelta(minutes=np.random.randint(0, int((end_date - start_date).total_seconds())//60)) for _ in range(num_samples)]\n\n# # Generate random categorical data (vendor_id and store_and_fwd_flag)\n# vendor_ids = np.random.choice(['VENDOR_1', 'VENDOR_2'], num_samples)\n# store_and_fwd_flag = np.random.choice(['Y', 'N'], num_samples)\n\n# # Generate random discrete data (passenger_count)\n# passenger_count = np.random.randint(1, 7, num_samples)\n\n# # Generate random trip duration as the target/label column\n# trip_duration = np.random.randint(300, 3600, num_samples)  # Random duration between 5 minutes to 1 hour\n\n# # Create a DataFrame to store the generated sample data\n# data = {\n#     'pickup_datetime': pickup_datetime,\n#     'pickup_latitude': pickup_latitude,\n#     'pickup_longitude': pickup_longitude,\n#     'dropoff_latitude': dropoff_latitude,\n#     'dropoff_longitude': dropoff_longitude,\n#     'vendor_id': vendor_ids,\n#     'store_and_fwd_flag': store_and_fwd_flag,\n#     'passenger_count': passenger_count,\n#     'trip_duration': trip_duration\n# }\n# df = pd.DataFrame(data)\n\n# # Print the first few rows of the generated sample data\n# print(df.head())\n```\n:::\n\n\n::: {#c39fc050 .cell execution_count=6}\n``` {.python .cell-code}\n# df.head() \n```\n:::\n\n\n::: {#6b7730e5 .cell execution_count=7}\n``` {.python .cell-code}\n# import numpy as np\n# import matplotlib.pyplot as plt\n# plt.rcParams[\"figure.figsize\"] = (5, 3) # (w, h)\n# plt.rcParams[\"figure.dpi\"] = 200\n\n\n# m = 100\n# x = 2 * np.random.rand(m)\n# y = 5 + 2 * x + np.random.randn(m)\n# plt.scatter(x, y);\n# from scipy.stats import linregress\n\n# slope,intercept=linregress(x,y)[:2]\n# print(f\"slope:{slope:.2f} intercept:{intercept:.2f}\")\n\n# def mseloss(theta,X,y):\n#     return -1 * (1/len(y))* np.sum((X@theta-y)**2) \n# x.shape\n\n# np.column_stack([np.ones(len(x)),x])\n\n# from scipy.stats import linregress \n\n# slope,intercept=linregress(x,y)[:2]\n# print(f\"slope:{slope:.2f} intercept:{intercept:.2f}\")\n\n# x = np.array([np.ones(m), x]).transpose()\n# def accuracy(x, y, theta):\n#     return - 1 / m * np.sum((np.dot(x, theta) - y) ** 2)\n\n# def gradient(x, y, theta):\n#     return -1 / m * x.T.dot(np.dot(x, theta) - y)\n\n# num_epochs = 500\n# learning_rate = 0.1\n\n# def train(x, y):\n#     accs = []\n#     thetas = []\n#     theta = np.zeros(2)\n#     for _ in range(num_epochs):\n#         # keep track of accuracy and theta over time\n#         acc = accuracy(x, y, theta)\n#         thetas.append(theta)\n#         accs.append(acc)\n        \n#         # update theta\n#         theta = theta + learning_rate * gradient(x, y, theta)\n        \n#     return theta, thetas, accs\n\n# theta, thetas, accs = train(x, y)\n# print(f\"slope: {theta[1]:.3f}, intercept: {theta[0]:.3f}\")\n# slope: 1.899, intercept: 5.128  \n\n# plt.plot(accs)\n# plt.xlabel('Epoch Number')\n# plt.ylabel('Accuracy');\n\n\n# from mpl_toolkits.mplot3d import Axes3D\n# i = np.linspace(-10, 20, 50)\n# j = np.linspace(-10, 20, 50)\n# i, j = np.meshgrid(i, j)\n# k = np.array([accuracy(x, y, th) for th in zip(np.ravel(i), np.ravel(j))]).reshape(i.shape)\n# fig = plt.figure(figsize=(9,6))\n# ax = fig.gca(projection='3d')\n# ax.plot_surface(i, j, k, alpha=0.2)\n# ax.plot([t[0] for t in thetas], [t[1] for t in thetas], accs, marker=\"o\", markersize=3, alpha=0.1);\n# ax.set_xlabel(r'$\\theta_0$'); ax.set_ylabel(r'$\\theta_1$')\n# ax.set_zlabel(\"Accuracy\");\n```\n:::\n\n\n::: {#4e533340 .cell execution_count=8}\n``` {.python .cell-code}\n# import duckdb\n\n# with duckdb.connect(\":memory:\") as con:\n#     con.execute(\"SET s3_endpoint='storage.googleapis.com'\")\n#     updates = con.execute(f\"\"\"\n#     SELECT *\n#     FROM READ_PARQUET('s3://bike-sharing-history/toulouse/jcdecaux/*/*.parquet');\n#     \"\"\").fetch_df()  # this is a pandas DataFrame\n\n# updates.head()\n\n# history = (\n#     updates\n#     .groupby('station')\n#     .resample(\n#         rule='15min',\n#         on='commit_at'\n#     ).max()\n#     .drop(columns='station')\n#     .groupby('station').shift(1)\n#     .groupby('station').ffill()\n#     .dropna(subset=['skipped_updates'])\n#     [['bikes', 'stands']].astype('uint8')\n# )\n\n\n\n\n# bikes_ahead = (\n#     history\n#     .groupby('station')['bikes'].shift([-i for i in range(1, 9)])\n#     .dropna()\n#     .astype('uint8')\n# )\n# bikes_ahead.columns = [f'bikes_+{15 * i}min' for i in range(1, 9)]\n\n# print(\n#     bikes_ahead\n#     .query(\"station == '00003 - POMME'\")\n#     .head()\n#     .to_markdown()\n# )\n\n# history.loc[bikes_ahead.index]\n\n# from sklearn import ensemble\n# from sklearn import model_selection\n\n# model = ensemble.HistGradientBoostingRegressor(\n#     loss='poisson',\n#     random_state=42\n# )\n# features = history.loc[bikes_ahead.index]\n\n# cv = model_selection.GroupKFold(n_splits=5)\n# groups = bikes_ahead.index.get_level_values('station')\n\n# features.head()\n# bikes_ahead.columns\n\n\n# features = history.loc[bikes_ahead.index]\n\n# cv = model_selection.GroupKFold(n_splits=5)\n# groups = bikes_ahead.index.get_level_values('station')\n\n# for target_col in bikes_ahead.columns:\n#     scores = model_selection.cross_val_score(\n#         model,\n#         features,\n#         bikes_ahead[target_col],\n#         groups=groups,\n#         cv=cv,\n#         scoring='neg_mean_squared_error'\n#     )\n#     print(f'{target_col} — {-scores.mean():.2f} ± {scores.std():.2f}')\n\n\n# import numpy as np\n# import scipy as sp \n# from joblib import dump\n# from pathlib import Path\n# import polars as pl\n\n# def calcbytes(s):\n#     arr=np.zeros((s,1))\n#     arr[s-1]=1\n#     dump(arr,'tmp.pickle')\n#     return Path('tmp.pickle').stat().st_size\n\n# s = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n# b= [ calcbytes(s) for _ in s]\n# b\n\n# s\n```\n:::\n\n\n1. Setting Environment Variables:  \n`nano .zshenv`\n` export EIA_API_KEY = '' `\n\n2. \n\nData pipeline \nProcess of moving data from one data source to another\nETL \n\nScope: \n1. Hourly demand for electricy by subregrion \n2. All subregions unded california system opertor: \n    a) Pacific Gas and Electric \n    b) San Diego Gas ane Electric \n    c) Southern California Edison \n    d) Valley Electric Association \n3. Refresh daily \n\nRequirement: \n1. Fully automated \n2. High level of customization\n3. Data quality and unit tests \n4. Monitor \n\nData pipeline Architecture: \n\n<!-- ![Caption for the picture.](/path/to/image.png) -->\n\n1. Data processing \n2. Create and udpate metadata \n3. Append data to the normalized table \nhttps://api.eia.gov/v2/electricity/rto/region-sub-ba-data/data/?frequency=hourly&data[0]=value&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=fwfOQIBXl0JIpVhL0QveMOuWmtIs169EP72zugF1\n\n\n\nData backfill - runned locally\n\n\nData Quality - unit tests \n\nDeterministic \n1. Data structure and attributes\n2. Field names \n3. Value ranges \n4. Duplicates\n\nNon-Deterministic\n\n1. Missing values\n2. Value ranges\n3. Delays \n\nGithub Actions\n\nTrigger vs Scheduled Action\n\nGithub secrets \nLogs \nDocker \n\nWe need a YAML \nOS \nDocker - reproducable environment\nworkflow \n\n.github/workflow\n\n::: {#0572cfca .cell execution_count=9}\n``` {.python .cell-code}\nimport requests \n```\n:::\n\n\nI was tasked with creating a view that displayed the total scan count for each employee along with the date of their latest scan. Here's a glimpse of what I encountered:\n\nGiven a demo table 'employees' with columns:\n\nemployee_id (int)\nemployee_card_scan_count (int)\nemployee_card_scan_date (string)\n\nThe desired view 'employee_scan_count' was to contain:\n\nemployee_id\ntotal_scans\nlatest_scan_date\n\nIn my exploration, I first attempted to find the total scan count for each employee:\n\nQuery 1:\n\nSELECT employee_id, COUNT(employee_card_scan_count) AS total_scans\nFROM employees\nGROUP BY employee_id\nThis query executed in around a minute.\n\nHowever, when I added the additional requirement to find the latest scan date:\n\nQuery 2:\n\nSELECT employee_id, SUM(employee_card_scan_count) AS total_scans, MAX(employee_card_scan_date) AS latest_scan_date\nFROM employees\nGROUP BY employee_id\n\nI was astonished to find that this query took nearly 5 minutes to run – five times the time taken by Query 1.\n\nAfter delving deeper into the internals of Spark SQL, I uncovered the reason behind this stark difference in execution times:\n\nIn Query 1, Spark utilized a hash table to store 'employee_id' as the key and updated it whenever the same key was encountered during table scanning. This resulted in a time complexity of O(n).\n\nHowever, in Query 2, when Spark attempted the same mechanism for the key-value pair of {employee_id, total_scans, latest_scan_date}, it encountered a bottleneck. Since 'latest_scan_date' was of type string and strings are immutable, Spark couldn't perform updates on the value efficiently. Consequently, it employed a technique called sort aggregation. This involved sorting the entire table based on 'employee_id' before performing the group by operation, resulting in a time complexity of O(nlog(n)).\n\nIn the realm of big data, the impact of this logarithmic complexity can be significant.\n\nFor instance, with a total of n = 100,000 rows:\n\nO(n) = 100,000\nO(nlog(n)) = 100,000 * log(100,000) ≈ 500,000\nHence, the additional execution time for Query 2 was attributed to this discrepancy in complexity.\n\nFor your knowledge, query 1 used Hash aggregation mechanism and query 2 used Sort aggregation.\n\nWith this puzzle solved, I was able to rest easy, content with a deeper understanding of Spark SQL's optimization strategies – and a well-deserved three-hour nap, as it was already 6 AM.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}